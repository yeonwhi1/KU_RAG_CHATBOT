import streamlit as st
import tempfile
import os
import traceback 

# [ë³€ê²½ 1] í™˜ê²½ ë³€ìˆ˜ ë¡œë“œë¥¼ ìœ„í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸
from dotenv import load_dotenv

# LangChain ê´€ë ¨ ëª¨ë“ˆ ì„í¬íŠ¸
from langchain_community.document_loaders import PyPDFLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_openai import OpenAIEmbeddings
from langchain_community.vectorstores import FAISS
from langchain_openai import ChatOpenAI
from langchain_core.prompts import PromptTemplate
from langchain_core.output_parsers import StrOutputParser
from langchain_core.runnables import RunnablePassthrough

# [ë³€ê²½ 2] .env íŒŒì¼ í™œì„±í™” (ë¡œì»¬ ê°œë°œ ì‹œ .env íŒŒì¼ì—ì„œ í‚¤ë¥¼ ì½ì–´ì˜´)
load_dotenv()

# 1. í˜ì´ì§€ ê¸°ë³¸ ì„¤ì •
st.set_page_config(page_title="ë‚˜ë§Œì˜ RAG ì±—ë´‡", page_icon="ğŸ»")
st.title("ğŸ» PDF ê¸°ë°˜ RAG ì±—ë´‡")

# [ë³€ê²½ 3] API KEY ì…ë ¥ì°½ ì œê±° -> í™˜ê²½ ë³€ìˆ˜ì—ì„œ ê°€ì ¸ì˜¤ê¸°
openai_api_key = os.getenv("OPENAI_API_KEY")

# API Keyê°€ ì—†ëŠ” ê²½ìš° ê²½ê³  í‘œì‹œ ë° ì¤‘ë‹¨
if not openai_api_key:
    st.error("í™˜ê²½ ë³€ìˆ˜ `OPENAI_API_KEY`ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. .env íŒŒì¼ì´ë‚˜ ì‹œìŠ¤í…œ í™˜ê²½ ë³€ìˆ˜ë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”.")
    st.stop()  # í‚¤ê°€ ì—†ìœ¼ë©´ ì•± ì‹¤í–‰ì„ ì—¬ê¸°ì„œ ë©ˆì¶¤

st.markdown("---")

# ì‚¬ì´ë“œë°”: ì„¤ì • ë° ì…ë ¥
with st.sidebar:
    st.header("ì„¤ì • (Configuration)")
    
    # 2. ë¬¸ì„œ ì—…ë¡œë“œ ë° ì¹´í…Œê³ ë¦¬ ì„ íƒ
    st.subheader("ë¬¸ì„œ ì—…ë¡œë“œ & ì„ íƒ")
    uploaded_files = st.file_uploader("PDF íŒŒì¼ì„ ì—…ë¡œë“œí•˜ì„¸ìš”", type=["pdf"], accept_multiple_files=True)
    
    # ì—…ë¡œë“œëœ íŒŒì¼ì´ ìˆì„ ê²½ìš° ì„ íƒ ë°•ìŠ¤ í™œì„±í™”
    selected_doc = None
    if uploaded_files:
        doc_names = [file.name for file in uploaded_files]
        selected_doc_name = st.selectbox("ê²€ìƒ‰í•  ë¬¸ì„œë¥¼ ì„ íƒí•˜ì„¸ìš” (ì¹´í…Œê³ ë¦¬)", doc_names)
        
        # ì„ íƒëœ íŒŒì¼ ê°ì²´ ì°¾ê¸°
        for file in uploaded_files:
            if file.name == selected_doc_name:
                selected_doc = file
                break
    
    st.markdown("---")
    
    # 5. ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ ì„¤ì • (ì‚¬ìš©ì ì…ë ¥ ê°€ëŠ¥)
    st.subheader("ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ ì„¤ì •")
    default_system_prompt = """ë‹¹ì‹ ì€ ì§ˆë¬¸ì— ë‹µë³€í•˜ëŠ” ì‘ì—…ì„ ìˆ˜í–‰í•˜ëŠ” ì¹œì ˆí•œ ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤.
ë‹¤ìŒì— ì œê³µëœ ë¬¸ë§¥ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì§ˆë¬¸ì— ë‹µí•˜ì„¸ìš”.
ì •ë‹µì„ ëª¨ë¥¼ ê²½ìš°, ëª¨ë¥¸ë‹¤ê³ ë§Œ ë§í•˜ì„¸ìš”.
ë‹µë³€ì€ ë°˜ë“œì‹œ í•œêµ­ì–´ë¡œ ì‘ì„±í•˜ì„¸ìš”."""
    
    system_prompt_input = st.text_area("AIì—ê²Œ ë¶€ì—¬í•  ì—­í• /ì§€ì‹œì‚¬í•­", value=default_system_prompt, height=200)
    
    process_btn = st.button("ë¬¸ì„œ ì²˜ë¦¬ ë° ì±—ë´‡ ì´ˆê¸°í™”")

# ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™” (ì±„íŒ… ê¸°ë¡, ë²¡í„° ì €ì¥ì†Œ ë“±)
if "messages" not in st.session_state:
    st.session_state["messages"] = []
if "vectorstore" not in st.session_state:
    st.session_state["vectorstore"] = None

# --- LangChain êµ¬ì„± ìš”ì†Œ í•¨ìˆ˜í™” (ëª¨ë“ˆ ë¶„ë¦¬) ---

def process_pdf(file):
    """
    [Document Load] Streamlit ì—…ë¡œë“œ íŒŒì¼ì„ ì„ì‹œ ì €ì¥ í›„ PyPDFLoaderë¡œ ë¡œë“œ
    """
    with tempfile.NamedTemporaryFile(delete=False, suffix=".pdf") as tmp_file:
        tmp_file.write(file.getvalue())
        tmp_path = tmp_file.name
        
    loader = PyPDFLoader(tmp_path)
    docs = loader.load()
    os.remove(tmp_path) # ì„ì‹œ íŒŒì¼ ì‚­ì œ
    return docs

def split_text(docs):
    """
    [Text Split] ë¬¸ì„œë¥¼ ì²­í¬ ë‹¨ìœ„ë¡œ ë¶„í• 
    """
    text_splitter = RecursiveCharacterTextSplitter(
        separators=["\n\n", "\n", ". ", " "],
        chunk_size=300, 
        chunk_overlap=50
    )
    return text_splitter.split_documents(docs)

def create_vectorstore(chunks):
    """
    [Embedding & VectorStore] ì„ë² ë”© ìƒì„± ë° FAISS ì €ì¥ì†Œ êµ¬ì¶•
    [ë³€ê²½ 4] openai_api_key ì¸ì ì œê±° (LangChainì´ í™˜ê²½ë³€ìˆ˜ë¥¼ ìë™ ì¸ì‹í•¨, í˜¹ì€ ì „ì—­ ë³€ìˆ˜ ì‚¬ìš©)
    """
    embeddings = OpenAIEmbeddings(
        model="text-embedding-3-small", 
        openai_api_key=openai_api_key  # ì „ì—­ ë³€ìˆ˜ í˜¹ì€ í™˜ê²½ë³€ìˆ˜ ì‚¬ìš©
    )
    vectorstore = FAISS.from_documents(chunks, embeddings)
    return vectorstore

def get_rag_chain(vectorstore, system_prompt):
    """
    [Chain] Retriever, Prompt, LLM ì—°ê²°
    [ë³€ê²½ 5] api_key ì¸ì ì œê±°
    """
    # 1. Retriever ì„¤ì • (MMR ë°©ì‹)
    retriever = vectorstore.as_retriever(
        search_type="mmr",
        search_kwargs={"k": 3, "lambda_mult": 0.8}
    )

    # 2. Prompt Template ì„¤ì •
    template = system_prompt + "\n\n#ë¬¸ë§¥:\n{context}\n\n#ì§ˆë¬¸:\n{question}\n\n#ë‹µë³€:"
    prompt = PromptTemplate.from_template(template)

    # 3. LLM ì„¤ì •
    llm = ChatOpenAI(
        model_name="gpt-4o-mini", 
        temperature=0,
        openai_api_key=openai_api_key # ì „ì—­ ë³€ìˆ˜ ì‚¬ìš©
    )

    # 4. Chain êµ¬ì„± (LCEL ë¬¸ë²•)
    chain = (
        {
            "context": retriever,
            "question": RunnablePassthrough()
        }
        | prompt
        | llm
        | StrOutputParser()
    )
    return chain

# --- ë©”ì¸ ë¡œì§ ì‹¤í–‰ ---

# ë²„íŠ¼ì´ ëˆŒë¦¬ë©´ ë¬¸ì„œ ì²˜ë¦¬ ì‹œì‘
if process_btn:
    # [ë³€ê²½ 6] API Key ìœ íš¨ì„± ê²€ì‚¬ ë¡œì§ ì œê±° (ìœ„ì—ì„œ st.stopìœ¼ë¡œ ì´ë¯¸ ì²˜ë¦¬ë¨)
    if not selected_doc:
        st.error("ë¬¸ì„œë¥¼ ì—…ë¡œë“œí•˜ê³  ì„ íƒí•´ì£¼ì„¸ìš”.")
    else:
        with st.spinner(f"'{selected_doc.name}' ë¬¸ì„œë¥¼ ë¶„ì„ ì¤‘ì…ë‹ˆë‹¤..."):
            try:
                # 1. Document Load
                raw_docs = process_pdf(selected_doc)
                # 2. Text Split
                chunks = split_text(raw_docs)
                # 3. Embedding & VectorStore
                # [ë³€ê²½ 7] ì¸ì ì „ë‹¬ ë°©ì‹ ê°„ì†Œí™”
                vectorstore = create_vectorstore(chunks)
                
                # ì„¸ì…˜ì— ì €ì¥
                st.session_state["vectorstore"] = vectorstore
                
                # ì±„íŒ… ê¸°ë¡ ì´ˆê¸°í™”
                st.session_state["messages"] = [{"role": "assistant", "content": "ë¬¸ì„œ ë¶„ì„ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤! ì§ˆë¬¸í•´ì£¼ì„¸ìš”."}]
                st.success("ì™„ë£Œ!")
            except Exception as e:
                st.error(f"ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}")
                st.code(traceback.format_exc())

# ì±„íŒ… ì¸í„°í˜ì´ìŠ¤
# 1. ì´ì „ ëŒ€í™” ì¶œë ¥
for msg in st.session_state["messages"]:
    st.chat_message(msg["role"]).write(msg["content"])

# 2. ì‚¬ìš©ì ì…ë ¥ ì²˜ë¦¬
if query := st.chat_input("ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”..."):
    # ì‚¬ìš©ì ì§ˆë¬¸ í‘œì‹œ
    st.session_state["messages"].append({"role": "user", "content": query})
    st.chat_message("user").write(query)

    # ë‹µë³€ ìƒì„±
    if st.session_state["vectorstore"] is not None:
        try:
            # Chain ìƒì„±
            # [ë³€ê²½ 8] ì¸ì ì „ë‹¬ ë°©ì‹ ê°„ì†Œí™”
            rag_chain = get_rag_chain(
                st.session_state["vectorstore"], 
                system_prompt_input
            )
            
            with st.chat_message("assistant"):
                with st.spinner("ìƒê° ì¤‘..."):
                    response = rag_chain.invoke(query)
                    st.write(response)
            
            # ë‹µë³€ ì €ì¥
            st.session_state["messages"].append({"role": "assistant", "content": response})
        except Exception as e:
            st.error(f"ë‹µë³€ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}")
    else:
        st.warning("ë¨¼ì € ë¬¸ì„œë¥¼ ì—…ë¡œë“œí•˜ê³  'ì´ˆê¸°í™”' ë²„íŠ¼ì„ ëˆŒëŸ¬ì£¼ì„¸ìš”.")